{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from glob import glob\n",
    "import rioxarray as rxr\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "import rasterio.features\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import importlib.util\n",
    "from rioxarray.merge import merge_arrays\n",
    "from scipy.optimize import minimize\n",
    "from scipy.signal import find_peaks\n",
    "import warnings\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2\n",
    "from multiprocessing import Pool\n",
    "warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "#import clear output\n",
    "from IPython.display import clear_output\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# istarmap.py for Python <3.8\n",
    "\n",
    "\n",
    "\n",
    "def recreate_dir(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def load_config(path):\n",
    "    spec = importlib.util.spec_from_file_location(\"CFG\", path)\n",
    "    CFG = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(CFG)\n",
    "    return CFG\n",
    "\n",
    "def inline_chart(values, width=10):\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    #if len(values) > width resample to 10 values\n",
    "    if len(values) > width:\n",
    "        values = np.array_split(values, width)\n",
    "        values = [np.mean(v) for v in values]\n",
    "    else:\n",
    "        #pad with min value\n",
    "        values = np.pad(values, (0, width-len(values)), \"constant\", constant_values=min_val)\n",
    "    #normalize\n",
    "    values = (values - min_val)/(max_val - min_val)\n",
    "    chart = \"\"\n",
    "    for value in values:\n",
    "        if value >= 0. and value < 0.125:\n",
    "            chart+=\"▁\"\n",
    "        elif value >= 0.125 and value < 0.25:\n",
    "            chart+=\"▂\"\n",
    "        elif value >= 0.25 and value < 0.375:\n",
    "            chart+=\"▃\"\n",
    "        elif value >= 0.375 and value < 0.5:\n",
    "            chart+=\"▄\"\n",
    "        elif value >= 0.5 and value < 0.625:\n",
    "            chart+=\"▅\"\n",
    "        elif value >= 0.625 and value < 0.75:\n",
    "            chart+=\"▆\"\n",
    "        elif value >= 0.75 and value < 0.875:\n",
    "            chart+=\"▇\"\n",
    "        elif value >= 0.875 and value <= 1.:\n",
    "            chart+=\"▉\"\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/grodzisko_20230111\"\n",
    "CFG = load_config(f\"{DATA_DIR}/config.py\").CALIBRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure logging to file\n",
    "import logging\n",
    "log_path = f\"{DATA_DIR}/logs/calibration_{datetime.now().strftime('%d%m%Y%H%M%S')}.log\"\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "logging.basicConfig(filename=log_path,level=logging.INFO, format='%(asctime)s %(levelname)-8s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.handlers.clear()\n",
    "#logger.addHandler(logging.StreamHandler())\n",
    "logger.info(\"Starting procedure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "GEOTIF_DEVIGNETT_DIR does not exist. Please run 2_devignetting.ipynb first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39massert\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(TIF_DEVIGNETTE_DIR), \u001b[39m\"\u001b[39m\u001b[39mTIF_DEVIGNETT_DIR does not exist. Please run 2_devignetting.ipynb first.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m GEOTIF_DEVIGNETTE_DIR \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDATA_DIR\u001b[39m}\u001b[39;00m\u001b[39m/geotif_devignette\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[39massert\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(GEOTIF_DEVIGNETTE_DIR), \u001b[39m\"\u001b[39m\u001b[39mGEOTIF_DEVIGNETT_DIR does not exist. Please run 2_devignetting.ipynb first.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m TIF_CAL_DIR \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDATA_DIR\u001b[39m}\u001b[39;00m\u001b[39m/tif_cal\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m GEOTIF_CAL_DIR \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDATA_DIR\u001b[39m}\u001b[39;00m\u001b[39m/geotif_cal\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: GEOTIF_DEVIGNETT_DIR does not exist. Please run 2_devignetting.ipynb first."
     ]
    }
   ],
   "source": [
    "TMP_DIR = f\"{DATA_DIR}/tmp\"\n",
    "recreate_dir(TMP_DIR)\n",
    "TIF_DEVIGNETTE_DIR = f\"{DATA_DIR}/tif_devignette\"\n",
    "assert os.path.exists(TIF_DEVIGNETTE_DIR), \"TIF_DEVIGNETT_DIR does not exist. Please run 2_devignetting.ipynb first.\"\n",
    "GEOTIF_DEVIGNETTE_DIR = f\"{DATA_DIR}/geotif_devignette\"\n",
    "assert os.path.exists(GEOTIF_DEVIGNETTE_DIR), \"GEOTIF_DEVIGNETT_DIR does not exist. Please run 2_devignetting.ipynb first.\"\n",
    "\n",
    "TIF_CAL_DIR = f\"{DATA_DIR}/tif_cal\"\n",
    "GEOTIF_CAL_DIR = f\"{DATA_DIR}/geotif_cal\"\n",
    "\n",
    "# PLOT_CLIP_DIR = f\"{DATA_DIR}/plot_clip\"\n",
    "# PLOT_CAL_DIR = f\"{DATA_DIR}/plot_cal\"\n",
    "CACHE_DIR = f\"{DATA_DIR}/cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "# TEMP_OPTIM_DATASET_DIR = f\"{DATA_DIR}/temp_optim_dataset\"\n",
    "# I_CLIP_DIR = f\"{TEMP_OPTIM_DATASET_DIR}/i_clip\"\n",
    "# J_CLIP_DIR = f\"{TEMP_OPTIM_DATASET_DIR}/j_clip\"\n",
    "# CLIP_MASK_DIR = f\"{TEMP_OPTIM_DATASET_DIR}/clip_mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CACHE_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m CFG\u001b[39m.\u001b[39mCACHE \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mCACHE_DIR\u001b[39m}\u001b[39;00m\u001b[39m/footprints.pkl\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mLoading footprints from cache\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mCACHE_DIR\u001b[39m}\u001b[39;00m\u001b[39m/footprints.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CACHE_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "if CFG.CACHE and os.path.exists(f\"{CACHE_DIR}/footprints.pkl\"):\n",
    "    logger.info(\"Loading footprints from cache\")\n",
    "    with open(f\"{CACHE_DIR}/footprints.pkl\", \"rb\") as f:\n",
    "        footprints = pickle.load(f)\n",
    "else:\n",
    "    logger.info(\"Reading footprints from geotifs\")\n",
    "    geometries = []\n",
    "    names = []\n",
    "    timestamps = []\n",
    "    for path in tqdm(glob(f\"{GEOTIF_DEVIGNETTE_DIR}/*.tif\")):\n",
    "        raster = rxr.open_rasterio(path)\n",
    "        footprints = rasterio.features.shapes((raster != raster.rio.nodata).values.astype(np.int16), transform=raster.rio.transform())\n",
    "        footprints = [Polygon(geom[\"coordinates\"][0]).simplify(10) for geom, colval in footprints if colval == 1]\n",
    "        assert len(footprints) == 1, \"More than one footprint found\"\n",
    "        #example name DJI_20230303092123_0066_T.tif. Get timestamp\n",
    "        timestamp = os.path.basename(path).split(\"_\")[1]\n",
    "        timestamp = datetime.strptime(timestamp, \"%Y%m%d%H%M%S\")\n",
    "        timestamp = timestamp.timestamp()\n",
    "        \n",
    "        names.append(os.path.basename(path))\n",
    "        geometries.append(footprints[0])\n",
    "        timestamps.append(timestamp)\n",
    "    timestamps = np.array(timestamps)\n",
    "    timestamps = (timestamps - timestamps.min())\n",
    "    footprints = gpd.GeoDataFrame({\"name\": names, \"time\": timestamps, \"geometry\": geometries})\n",
    "    #write CRS\n",
    "    footprints.crs = CFG.CRS\n",
    "    with open(f\"{CACHE_DIR}/footprints.pkl\", \"wb\") as f:\n",
    "        pickle.dump(footprints, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nan_gaussian_filter(arr, sigma):\n",
    "#     \"\"\"Apply gaussian filter to array while ignoring nans\"\"\"\n",
    "#     V=arr.copy()\n",
    "#     V[np.isnan(arr)]=0\n",
    "#     VV=gaussian_filter(V,sigma=sigma)\n",
    "#     W=0*arr.copy()+1\n",
    "#     W[np.isnan(arr)]=0\n",
    "#     WW=gaussian_filter(W,sigma=sigma)\n",
    "#     Z=VV/WW\n",
    "#     Z[np.isnan(arr)]=np.nan\n",
    "#     return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CACHE_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m CFG\u001b[39m.\u001b[39mCACHE \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mCACHE_DIR\u001b[39m}\u001b[39;00m\u001b[39m/calibration_pairs.pkl\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mLoading cached pairs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mCACHE_DIR\u001b[39m}\u001b[39;00m\u001b[39m/calibration_pairs.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CACHE_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "if CFG.CACHE and os.path.exists(f\"{CACHE_DIR}/calibration_pairs.pkl\"):\n",
    "    logger.info(\"Loading cached pairs\")\n",
    "    with open(f\"{CACHE_DIR}/calibration_pairs.pkl\", \"rb\") as f:\n",
    "        pairs_i, pairs_j, pairs_area, i_clips, j_clips, masks, diff_vars = pickle.load(f)\n",
    "else:\n",
    "    logger.info(\"Generating temprature global optimization dataset\")\n",
    "    #erode footprints\n",
    "    if CFG.EROSION > 0:\n",
    "        footprints[\"geometry\"] = footprints[\"geometry\"].buffer(-CFG.EROSION)\n",
    "    # recreate_dir(TEMP_OPTIM_DATASET_DIR)\n",
    "    # recreate_dir(I_CLIP_DIR)\n",
    "    # recreate_dir(J_CLIP_DIR)\n",
    "    # recreate_dir(CLIP_MASK_DIR)\n",
    "    # recreate_dir(PLOT_CLIP_DIR)\n",
    "    pairs_i = []\n",
    "    pairs_j = []\n",
    "    pairs_area = []\n",
    "    i_clips = []\n",
    "    j_clips = []\n",
    "    masks = []\n",
    "    diff_vars = []\n",
    "    idx = 0\n",
    "    for i in tqdm(range(len(footprints))):\n",
    "        i_raster = rxr.open_rasterio(f\"{GEOTIF_DEVIGNETTE_DIR}/{footprints.iloc[i]['name']}\", masked=True)\n",
    "        for j in range(i+1, len(footprints)):\n",
    "            if footprints.iloc[i].geometry.intersects(footprints.iloc[j].geometry):\n",
    "                intersection = footprints.iloc[i].geometry.intersection(footprints.iloc[j].geometry)\n",
    "                if intersection.area < CFG.MIN_INTERSECTION_AREA:\n",
    "                    logger.info(f\"i ({i}), j ({j}): intersection area too small\")\n",
    "                    continue\n",
    "                j_raster = rxr.open_rasterio(f\"{GEOTIF_DEVIGNETTE_DIR}/{footprints.iloc[j]['name']}\", masked=True)\n",
    "                try:\n",
    "                    i_clip = i_raster.rio.clip([intersection])\n",
    "                    j_clip = j_raster.rio.clip([intersection])\n",
    "                except NoDataInBounds:\n",
    "                    logger.info(f\"i ({i}), j ({j}): NoDataInBounds\")\n",
    "                    continue\n",
    "                j_clip = j_clip.rio.reproject_match(i_clip)\n",
    "                i_clip = i_clip.values[0]\n",
    "                j_clip = j_clip.values[0]\n",
    "                diff = i_clip - j_clip\n",
    "                #variance of difference\n",
    "                diff_var = np.nanvar(diff)\n",
    "                #offset = np.nanmean(i_clip) - np.nanmean(j_clip)\n",
    "                #mse = np.nanmean((i_clip - j_clip)**2)\n",
    "                #rmse with offset compensated as a measure of pair alignment\n",
    "                # offset = np.nanmean(i_clip) - np.nanmean(j_clip)\n",
    "                \n",
    "                #i_clip = nan_gaussian_filter(i_clip, sigma=CFG.GAUSS_SIGMA)\n",
    "                #j_clip = nan_gaussian_filter(j_clip, sigma=CFG.GAUSS_SIGMA)\n",
    "                # i_var = np.nanvar(i_clip)\n",
    "                # j_var = np.nanvar(j_clip)\n",
    "                # var = np.nanmean([i_var, j_var])\n",
    "                # if np.isnan(var):\n",
    "                #     logger.info(f\"i ({i}), j ({j}): var is nan\")\n",
    "                #     continue\n",
    "                mask = (~np.isnan(i_clip) & ~np.isnan(j_clip)).astype(np.int16)\n",
    "                i_clip[np.isnan(i_clip)] = np.nanmean(i_clip)\n",
    "                j_clip[np.isnan(j_clip)] = np.nanmean(j_clip)\n",
    "                \n",
    "                # mask_copy = mask.copy()\n",
    "                # i_clip_copy = i_clip.copy()\n",
    "                # j_clip_copy = j_clip.copy()\n",
    "                \n",
    "                i_clip = cv2.resize(i_clip, (CFG.CLIP_SIZE, CFG.CLIP_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "                j_clip = cv2.resize(j_clip, (CFG.CLIP_SIZE, CFG.CLIP_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "                mask = cv2.resize(mask, (CFG.CLIP_SIZE, CFG.CLIP_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                #assert i_clip, j_clip, mask dont have nans\n",
    "                if np.isnan(i_clip).any():\n",
    "                    logger.info(f\"i ({i}), j ({j}): i_clip has nan\")\n",
    "                    continue\n",
    "                if np.isnan(j_clip).any():\n",
    "                    logger.info(f\"i ({i}), j ({j}): j_clip has nan\")\n",
    "                    continue\n",
    "                if np.isnan(mask).any():\n",
    "                    logger.info(f\"i ({i}), j ({j}): mask has nan\")\n",
    "                    continue\n",
    "                ###############################\n",
    "                # np.save(f\"{I_CLIP_DIR}/{idx}.npy\", i_clip_copy)\n",
    "                # np.save(f\"{J_CLIP_DIR}/{idx}.npy\", j_clip_copy)\n",
    "                # np.save(f\"{CLIP_MASK_DIR}/{idx}.npy\", mask_copy)\n",
    "                ###############################\n",
    "                # make plot with two images\n",
    "                # fig, ax = plt.subplots(2, 2, figsize=(10, 10), dpi=50)\n",
    "                # i_raster.plot(ax=ax[0][0])\n",
    "                # ax[0][0].plot(*intersection.exterior.xy, color=\"red\")\n",
    "                # j_raster.plot(ax=ax[0][1])\n",
    "                # ax[0][1].plot(*intersection.exterior.xy, color=\"red\")\n",
    "                # ax[1][0].imshow(i_clip, cmap=\"gray\")\n",
    "                # ax[1][1].imshow(j_clip, cmap=\"gray\")\n",
    "                # #save fig to file\n",
    "                # fig.savefig(f\"{PLOT_CLIP_DIR}/{i}_{j}.png\")\n",
    "                # plt.close()\n",
    "                ##############################\n",
    "                pairs_i.append(i)\n",
    "                pairs_j.append(j)\n",
    "                pairs_area.append(intersection.area)\n",
    "                i_clips.append(i_clip)\n",
    "                j_clips.append(j_clip)\n",
    "                masks.append(mask)\n",
    "                diff_vars.append(diff_var)\n",
    "                idx+=1\n",
    "                \n",
    "                # idx += 1\n",
    "    pairs_i = np.array(pairs_i)\n",
    "    pairs_j = np.array(pairs_j)\n",
    "    pairs_area = np.array(pairs_area)\n",
    "    i_clips = np.array(i_clips)\n",
    "    j_clips = np.array(j_clips)\n",
    "    masks = np.array(masks)\n",
    "    diff_vars = np.array(diff_vars)\n",
    "    #pickle dump\n",
    "    with open(f\"{CACHE_DIR}/calibration_pairs.pkl\", \"wb\") as f:\n",
    "        pickle.dump((pairs_i, pairs_j, pairs_area, i_clips, j_clips, masks, diff_vars), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_weights = pairs_area#np.emath.logn(10, pairs_area)\n",
    "#normalize from 0 to 1\n",
    "area_weights = area_weights/area_weights.max()#(area_weights - areights.min()) / (area_weights.max() - area_weights.min())\n",
    "#var_weights = np.emath.logn(10, 1/diff_vars)\n",
    "#var_weights = (var_weights - var_weights.min()) / (var_weights.max() - var_weights.min())\n",
    "#weights = area_weights * var_weights\n",
    "weights = area_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'footprints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#recreate_dir(PLOT_CLIP_DIR)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m n_images \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(footprints)\n\u001b[1;32m      4\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m pairs_i \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(pairs_i, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64, device\u001b[39m=\u001b[39mdevice) \u001b[39m# (n_pairs,)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'footprints' is not defined"
     ]
    }
   ],
   "source": [
    "#recreate_dir(PLOT_CLIP_DIR)\n",
    "\n",
    "n_images = len(footprints)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pairs_i = torch.tensor(pairs_i, dtype=torch.int64, device=device) # (n_pairs,)\n",
    "pairs_j = torch.tensor(pairs_j, dtype=torch.int64, device=device) # (n_pairs,)\n",
    "pairs_area = torch.tensor(pairs_area, dtype=torch.float32, device=device) # (n_pairs,)\n",
    "#pairs_var = torch.tensor(pairs_var, dtype=torch.float32, device=device) # (n_pairs,)\n",
    "i_clips = torch.tensor(i_clips, dtype=torch.float32, device=device) # (n_pairs, width, height)\n",
    "j_clips = torch.tensor(j_clips, dtype=torch.float32, device=device) # (n_pairs, width, height)\n",
    "masks = torch.tensor(masks, dtype=torch.float32, device=device) # (n_pairs, width, height)\n",
    "weights = torch.tensor(weights, dtype=torch.float32, device=device) # (n_pairs,)\n",
    "pairs_n_pixels = torch.sum(masks, dim=(1, 2)) # (n_pairs,)\n",
    "n_pixels = torch.sum(pairs_n_pixels) # (1,)\n",
    "#n_pixels_weighted = torch.sum(pairs_n_pixels * weights) # (1,)\n",
    "n_pairs = len(pairs_i) # (1,)\n",
    "\n",
    "i_clips_mean = torch.sum(i_clips * masks, dim=(1, 2)) / pairs_n_pixels # (n_pairs,)\n",
    "j_clips_mean = torch.sum(j_clips * masks, dim=(1, 2)) / pairs_n_pixels # (n_pairs,)\n",
    "i_clips_var = torch.sum(masks * (i_clips - i_clips_mean[:, None, None]) ** 2, dim=(1, 2)) / pairs_n_pixels # (n_pairs,)\n",
    "j_clips_var = torch.sum(masks * (j_clips - j_clips_mean[:, None, None]) ** 2, dim=(1, 2)) / pairs_n_pixels # (n_pairs,)\n",
    "a_coefs = torch.ones(n_images, dtype=torch.float32, device=device, requires_grad=False) # (n_images,)\n",
    "b_coefs = torch.zeros(n_images, dtype=torch.float32, device=device, requires_grad=True) # (n_images,)\n",
    "\n",
    "optimizer = torch.optim.Adam([b_coefs], lr=CFG.INIT_LR)#([a_coefs, b_coefs], lr=CFG.INIT_LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=CFG.LR_PLATEAU_FACTOR, patience=CFG.LR_PLATEAU_PATIENCE, cooldown=CFG.LR_PLATEAU_COOLDOWN, threshold=CFG.LR_PLATEAU_THRESHOLD, verbose=True)\n",
    "pairs_i_copy = pairs_i.clone()\n",
    "pairs_j_copy = pairs_j.clone()\n",
    "best_loss = np.inf\n",
    "es_counter = 0\n",
    "pixel_losses = []\n",
    "mean_losses = []\n",
    "var_losses = []\n",
    "for epoch in (pbar := tqdm(range(CFG.EPOCHS))):\n",
    "    #assert that pairs_i and pairs_j are equal to the original values\n",
    "    assert torch.all(pairs_i == pairs_i_copy)\n",
    "    assert torch.all(pairs_j == pairs_j_copy)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    optimizer.zero_grad()\n",
    "    i_clips_cal = a_coefs[pairs_i, None, None] * i_clips + b_coefs[pairs_i, None, None] # (n_pairs, width, height)\n",
    "    j_clips_cal = a_coefs[pairs_j, None, None] * j_clips + b_coefs[pairs_j, None, None] # (n_pairs, width, height)\n",
    "    i_clips_cal_mean = torch.sum(i_clips_cal * masks, dim=(1, 2)) / pairs_n_pixels # (n_pairs,)\n",
    "    j_clips_cal_mean = torch.sum(j_clips_cal * masks, dim=(1, 2)) / pairs_n_pixels # (n_pairs,)\n",
    "    i_clips_cal_var = torch.sum(masks * (i_clips_cal - i_clips_cal_mean[:, None, None]) ** 2, dim=(1, 2)) / pairs_n_pixels # (n_pairs,)\n",
    "    j_clips_cal_var = torch.sum(masks * (j_clips_cal - j_clips_cal_mean[:, None, None]) ** 2, dim=(1, 2)) / pairs_n_pixels # (n_pairs,)\n",
    "    #assert that i_clips_cal_mean and j_clips_cal_mean do not contain any NaNs\n",
    "\n",
    "    # #group i_clips_cal_mean by pairs_i and calculate the mean\n",
    "    # if epoch % 50 == 0:\n",
    "    #     xs = []\n",
    "    #     ys = []\n",
    "    #     for k in range(n_images):\n",
    "    #         for mean_val in i_clips_cal_mean.detach().cpu().numpy()[pairs_i == k]:\n",
    "    #             xs.append(k)\n",
    "    #             ys.append(mean_val)\n",
    "    #         for mean_val in j_clips_cal_mean.detach().cpu().numpy()[pairs_j == k]:\n",
    "    #             xs.append(k)\n",
    "    #             ys.append(mean_val)\n",
    "    #     plt.plot(xs, ys, \".\")\n",
    "    #     plt.show()\n",
    "    \n",
    "    pixel_loss = torch.sum((i_clips_cal - j_clips_cal)**2 * masks * weights[:, None, None])/n_pixels\n",
    "    #pixel_loss = torch.mean((i_clips_cal_mean - j_clips_cal_mean)**2)\n",
    "    # mean_loss = 0.000001  *(0.5/n_pairs)*(\n",
    "    #                 torch.sum((i_clips_cal_mean - i_clips_mean)**2 * weights[:, None, None]) + \n",
    "    #                 torch.sum((j_clips_cal_mean - j_clips_mean)**2 * weights[:, None, None])\n",
    "    #             )\n",
    "    #var_loss =  0.1   *(0.5/n_pairs)*(torch.sum(torch.abs(i_clips_cal_var - i_clips_var)**1)   + torch.sum(torch.abs(j_clips_cal_var - j_clips_var)**1))\n",
    "\n",
    "    loss = pixel_loss# + mean_loss# + var_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    \n",
    "    if best_loss-loss.item() > CFG.ES_MIN_DELTA:\n",
    "        best_loss = loss.item()\n",
    "        best_a_coefs = a_coefs.detach().cpu().numpy()\n",
    "        best_b_coefs = b_coefs.detach().cpu().numpy()\n",
    "        es_counter = 0\n",
    "    else:\n",
    "        es_counter += 1\n",
    "        if es_counter > CFG.ES_PATIENCE:\n",
    "            print(\"Early stopping\")\n",
    "            # for i_clip, j_clip, i, j, mask, a_i, b_i, a_j, b_j, i_clip_cal_mean, j_clip_cal_mean in zip(tqdm(i_clips_cal.detach().cpu().numpy()), j_clips_cal.detach().cpu().numpy(), pairs_i.detach().cpu().numpy(), pairs_j.detach().cpu().numpy(), masks.detach().cpu().numpy(), a_coefs[pairs_i].detach().cpu().numpy(), b_coefs[pairs_i].detach().cpu().numpy(), a_coefs[pairs_j].detach().cpu().numpy(), b_coefs[pairs_j].detach().cpu().numpy(), i_clips_cal_mean.detach().cpu().numpy(), j_clips_cal_mean.detach().cpu().numpy()):\n",
    "            #     fig, ax = plt.subplots(1,2, dpi=50)\n",
    "            #     ax[0].imshow(i_clip * mask)\n",
    "            #     ax[0].set_title(f\"{i} {i_clip_cal_mean}\")\n",
    "            #     ax[1].imshow(j_clip * mask)\n",
    "            #     ax[1].set_title(f\"{j} {j_clip_cal_mean}\")\n",
    "            #     fig.savefig(f\"{PLOT_CLIP_DIR}/{i}_{j}.png\")\n",
    "            #     plt.close(fig)\n",
    "            break\n",
    "\n",
    "    pixel_losses.append(pixel_loss.item())\n",
    "    #mean_losses.append(mean_loss.item())\n",
    "    #var_losses.append(var_loss.item())\n",
    "\n",
    "    pbar.set_postfix({\n",
    "        \"pixel_loss\": pixel_loss.item(), \n",
    "        #\"mean_loss\": mean_loss.item(),\n",
    "        #\"var_loss\": var_loss.item(),\n",
    "        \"loss\": f\"{loss.item()}\", \n",
    "        \"a_mean\": a_coefs.mean().item(), \n",
    "        \"b_mean\": b_coefs.mean().item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pixel_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#plot losses with log scale\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[39m.\u001b[39mplot(pixel_losses, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpixel_loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m#plt.plot(mean_losses, label=\"mean_loss\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39mplot(var_losses, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvar_loss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pixel_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot losses with log scale\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(pixel_losses, label=\"pixel_loss\")\n",
    "#plt.plot(mean_losses, label=\"mean_loss\")\n",
    "plt.plot(var_losses, label=\"var_loss\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recreate_dir(GEOTIF_CAL_DIR)\n",
    "recreate_dir(TIF_CAL_DIR)\n",
    "def save_calib(name, a, b):\n",
    "    geotif = rxr.open_rasterio(f\"{GEOTIF_DEVIGNETTE_DIR}/{name}\", masked=True)\n",
    "    geotif.data = geotif.data * a + b\n",
    "    geotif.rio.to_raster(f\"{GEOTIF_CAL_DIR}/{name}\")\n",
    "    \n",
    "\n",
    "with Pool(6) as pool:\n",
    "    #result = pool.istarmap(save_calib, zip(footprints[\"name\"].values, best_a_coefs, best_b_coefs))\n",
    "    result = list(tqdm(pool.istarmap(save_calib, zip(footprints[\"name\"].values, best_a_coefs, best_b_coefs)), total=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262/262 [01:16<00:00,  3.42it/s]\n"
     ]
    }
   ],
   "source": [
    "recreate_dir(GEOTIF_CAL_DIR)\n",
    "recreate_dir(TIF_CAL_DIR)\n",
    "for name, a, b in zip(tqdm(footprints[\"name\"]), best_a_coefs, best_b_coefs):\n",
    "    geotif = rxr.open_rasterio(f\"{GEOTIF_DEVIGNETTE_DIR}/{name}\", masked=True)\n",
    "    geotif.data = geotif.data * a + b\n",
    "    geotif.rio.to_raster(f\"{GEOTIF_CAL_DIR}/{name}\")\n",
    "    # tif = rxr.open_rasterio(f\"{TIF_DEVIGNETTE_DIR}/{name}\", masked=True)\n",
    "    # tif.data = tif.data * a + b\n",
    "    # tif.rio.to_raster(f\"{TIF_CAL_DIR}/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "import multiprocessing.pool as mpp\n",
    "def istarmap(self, func, iterable, chunksize=1):\n",
    "    \"\"\"starmap-version of imap\n",
    "    \"\"\"\n",
    "    self._check_running()\n",
    "    if chunksize < 1:\n",
    "        raise ValueError(\n",
    "            \"Chunksize must be 1+, not {0:n}\".format(\n",
    "                chunksize))\n",
    "\n",
    "    task_batches = mpp.Pool._get_tasks(func, iterable, chunksize)\n",
    "    result = mpp.IMapIterator(self)\n",
    "    self._taskqueue.put(\n",
    "        (\n",
    "            self._guarded_task_generation(result._job,\n",
    "                                          mpp.starmapstar,\n",
    "                                          task_batches),\n",
    "            result._set_length\n",
    "        ))\n",
    "    return (item for chunk in result for item in chunk)\n",
    "mpp.Pool.istarmap = istarmap\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import tqdm    \n",
    "\n",
    "def foo(a, b):\n",
    "    for _ in range(int(50e6)):\n",
    "        pass\n",
    "    return a, b    \n",
    "\n",
    "with Pool(4) as pool:\n",
    "    iterable = [(i, 'x') for i in range(10)]\n",
    "    for _ in tqdm.tqdm(pool.istarmap(foo, iterable),\n",
    "                        total=len(iterable)):\n",
    "        pass\n",
    "    \n",
    "def multiprocess(func, iterable, n_jobs=6):\n",
    "    with Pool(n_jobs) as pool:\n",
    "        for _ in tqdm.tqdm(pool.istarmap(foo, iterable),\n",
    "                        total=len(iterable)):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from multiprocessing import Pool\n",
    "import tqdm    \n",
    "\n",
    "def fun(a, b):\n",
    "    sleep(4)\n",
    "    return f\"{a} {b}\"\n",
    "l1 = [1,2,3, 4, 5, 6, 7, 8, 9, 10]\n",
    "l2 = [\"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\"]\n",
    "with Pool(6) as pool:\n",
    "    #result = pool.istarmap(save_calib, zip(footprints[\"name\"].values, best_a_coefs, best_b_coefs))\n",
    "    result = list(tqdm(pool.istarmap(fun, zip(l1, l2)), total=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
