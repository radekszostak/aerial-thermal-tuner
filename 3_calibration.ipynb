{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from glob import glob\n",
    "import rioxarray as rxr\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "import rasterio.features\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import importlib.util\n",
    "from rioxarray.merge import merge_arrays\n",
    "from scipy.optimize import minimize\n",
    "from scipy.signal import find_peaks\n",
    "import warnings\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2\n",
    "warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "def recreate_dir(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def load_config(path):\n",
    "    spec = importlib.util.spec_from_file_location(\"CFG\", path)\n",
    "    CFG = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(CFG)\n",
    "    return CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/rybna_202203241528\"\n",
    "CFG = load_config(f\"{DATA_DIR}/config.py\").CALIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure logging to file\n",
    "import logging\n",
    "log_path = f\"{DATA_DIR}/logs/calibration_{datetime.now().strftime('%d%m%Y%H%M%S')}.log\"\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "logging.basicConfig(filename=log_path,level=logging.INFO, format='%(asctime)s %(levelname)-8s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.handlers.clear()\n",
    "#logger.addHandler(logging.StreamHandler())\n",
    "logger.info(\"Starting procedure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_DIR = f\"{DATA_DIR}/tmp\"\n",
    "recreate_dir(TMP_DIR)\n",
    "TIFF_DIR = f\"{DATA_DIR}/tiff\"\n",
    "assert os.path.exists(TIFF_DIR), \"tiff_dir does not exist. Please run 1_conversion.ipynb first.\"\n",
    "GEOTIFF_OPTIM_DIR = f\"{DATA_DIR}/geotiff_optim\"\n",
    "assert os.path.exists(GEOTIFF_OPTIM_DIR), \"geotiff_dir does not exist. Please run 1_conversion.ipynb first.\"\n",
    "TIFF_CAL_DIR = f\"{DATA_DIR}/tiff_cal\"\n",
    "GEOTIFF_CAL_DIR = f\"{DATA_DIR}/geotiff_cal\"\n",
    "PLOT_CLIP_DIR = f\"{DATA_DIR}/plot_clip\"\n",
    "PLOT_CAL_DIR = f\"{DATA_DIR}/plot_cal\"\n",
    "TEMP_OPTIM_DATASET_DIR = f\"{DATA_DIR}/temp_optim_dataset\"\n",
    "I_CLIP_DIR = f\"{TEMP_OPTIM_DATASET_DIR}/i_clip\"\n",
    "J_CLIP_DIR = f\"{TEMP_OPTIM_DATASET_DIR}/j_clip\"\n",
    "CLIP_MASK_DIR = f\"{TEMP_OPTIM_DATASET_DIR}/clip_mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.CACHE and os.path.exists(f\"{DATA_DIR}/footprints.pkl\"):\n",
    "    logger.info(\"Loading footprints from cache\")\n",
    "    with open(f\"{DATA_DIR}/footprints.pkl\", \"rb\") as f:\n",
    "        footprints = pickle.load(f)\n",
    "else:\n",
    "    logger.info(\"Reading footprints from geotiffs\")\n",
    "    geometries = []\n",
    "    names = []\n",
    "    for path in tqdm(glob(f\"{GEOTIFF_OPTIM_DIR}/*.tiff\")):\n",
    "        raster = rxr.open_rasterio(path)\n",
    "        footprints = rasterio.features.shapes((raster != raster.rio.nodata).values.astype(np.int16), transform=raster.rio.transform())\n",
    "        footprints = [Polygon(geom[\"coordinates\"][0]).simplify(10) for geom, colval in footprints if colval == 1]\n",
    "        assert len(footprints) == 1, \"More than one footprint found\"\n",
    "        names.append(os.path.basename(path))\n",
    "        geometries.append(footprints[0])\n",
    "    footprints = gpd.GeoDataFrame({\"name\": names, \"geometry\": geometries})\n",
    "    #write CRS\n",
    "    footprints.crs = CFG.CRS\n",
    "    with open(f\"{DATA_DIR}/footprints.pkl\", \"wb\") as f:\n",
    "        pickle.dump(footprints, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erode footprints\n",
    "footprints[\"geometry\"] = footprints[\"geometry\"].buffer(-CFG.EROSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_gaussian_filter(arr, sigma):\n",
    "    \"\"\"Apply gaussian filter to array while ignoring nans\"\"\"\n",
    "    V=arr.copy()\n",
    "    V[np.isnan(arr)]=0\n",
    "    VV=gaussian_filter(V,sigma=sigma)\n",
    "    W=0*arr.copy()+1\n",
    "    W[np.isnan(arr)]=0\n",
    "    WW=gaussian_filter(W,sigma=sigma)\n",
    "    Z=VV/WW\n",
    "    Z[np.isnan(arr)]=np.nan\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [51:42<00:00,  6.89s/it]  \n"
     ]
    }
   ],
   "source": [
    "if CFG.CACHE and os.path.exists(f\"{TEMP_OPTIM_DATASET_DIR}/pairs.pkl\"):\n",
    "    logger.info(\"Loading cached pairs\")\n",
    "    with open(f\"{TEMP_OPTIM_DATASET_DIR}/pairs.pkl\", \"rb\") as f:\n",
    "        pairs_i, pairs_j, pairs_area, pairs_std = pickle.load(f)\n",
    "else:\n",
    "    logger.info(\"Generating temprature global optimization dataset\")\n",
    "    recreate_dir(TEMP_OPTIM_DATASET_DIR)\n",
    "    recreate_dir(I_CLIP_DIR)\n",
    "    recreate_dir(J_CLIP_DIR)\n",
    "    recreate_dir(CLIP_MASK_DIR)\n",
    "    pairs_i = []\n",
    "    pairs_j = []\n",
    "    pairs_area = []\n",
    "    pairs_std = []\n",
    "    idx = 0\n",
    "    for i in tqdm(range(len(footprints))):\n",
    "        i_raster = rxr.open_rasterio(f\"{GEOTIFF_OPTIM_DIR}/{footprints.iloc[i]['name']}\", masked=True)\n",
    "        for j in range(i+1, len(footprints)):\n",
    "            if footprints.iloc[i].geometry.intersects(footprints.iloc[j].geometry):\n",
    "                intersection = footprints.iloc[i].geometry.intersection(footprints.iloc[j].geometry)\n",
    "                if intersection.area < CFG.MIN_INTERSECTION_AREA:\n",
    "                    logger.info(f\"i ({i}), j ({j}): intersection area too small\")\n",
    "                    continue\n",
    "                j_raster = rxr.open_rasterio(f\"{GEOTIFF_OPTIM_DIR}/{footprints.iloc[j]['name']}\", masked=True)\n",
    "                try:\n",
    "                    i_clip = i_raster.rio.clip([intersection])\n",
    "                    j_clip = j_raster.rio.clip([intersection])\n",
    "                except NoDataInBounds:\n",
    "                    logger.info(f\"i ({i}), j ({j}): NoDataInBounds\")\n",
    "                    continue\n",
    "                j_clip = j_clip.rio.reproject_match(i_clip)\n",
    "                i_clip = i_clip.values[0]\n",
    "                j_clip = j_clip.values[0]\n",
    "                i_clip = nan_gaussian_filter(i_clip, sigma=CFG.GAUSS_SIGMA)\n",
    "                j_clip = nan_gaussian_filter(j_clip, sigma=CFG.GAUSS_SIGMA)\n",
    "                i_std = np.nanstd(i_clip)\n",
    "                j_std = np.nanstd(j_clip)\n",
    "                std = np.nanmean([i_std, j_std])\n",
    "                if np.isnan(std):\n",
    "                    logger.info(f\"i ({i}), j ({j}): std is nan\")\n",
    "                    continue\n",
    "                mask = (~np.isnan(i_clip) & ~np.isnan(j_clip)).astype(np.int16)\n",
    "                i_clip[np.isnan(i_clip)] = np.nanmean(i_clip)\n",
    "                j_clip[np.isnan(j_clip)] = np.nanmean(j_clip)\n",
    "                i_clips = []\n",
    "                j_clips = []\n",
    "                masks = []\n",
    "                sizes = [256, 128, 64, 32]\n",
    "                for size in sizes:\n",
    "                    i_clips.append(cv2.resize(i_clip, (size, size), interpolation=cv2.INTER_LINEAR))\n",
    "                    j_clips.append(cv2.resize(j_clip, (size, size), interpolation=cv2.INTER_LINEAR))\n",
    "                    masks.append(cv2.resize(mask, (size, size), interpolation=cv2.INTER_NEAREST))\n",
    "                    #assert i_clip, j_clip, mask dont have nans\n",
    "                    if np.isnan(i_clips[-1]).any():\n",
    "                        logger.info(f\"i ({i}), j ({j}): i_clip has nan\")\n",
    "                        continue\n",
    "                    if np.isnan(j_clips[-1]).any():\n",
    "                        logger.info(f\"i ({i}), j ({j}): j_clip has nan\")\n",
    "                        continue\n",
    "                    if np.isnan(masks[-1]).any():\n",
    "                        logger.info(f\"i ({i}), j ({j}): mask has nan\")\n",
    "                        continue\n",
    "                for i_clip, j_clip, mask, size in zip(i_clips, j_clips, masks, sizes):\n",
    "                    np.save(f\"{I_CLIP_DIR}/{idx}_{size}.npy\", i_clip)\n",
    "                    np.save(f\"{J_CLIP_DIR}/{idx}_{size}.npy\", j_clip)\n",
    "                    np.save(f\"{CLIP_MASK_DIR}/{idx}_{size}.npy\", mask)\n",
    "                pairs_i.append(i)\n",
    "                pairs_j.append(j)\n",
    "                pairs_area.append(intersection.area)\n",
    "                pairs_std.append(std)\n",
    "                idx += 1\n",
    "    pairs_i = np.array(pairs_i)\n",
    "    pairs_j = np.array(pairs_j)\n",
    "    pairs_area = np.array(pairs_area)\n",
    "    pairs_std = np.array(pairs_std)\n",
    "    #pickle dump\n",
    "    with open(f\"{TEMP_OPTIM_DATASET_DIR}/pairs.pkl\", \"wb\") as f:\n",
    "        pickle.dump((pairs_i, pairs_j, pairs_area, pairs_std), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch dataset\n",
    "class TempOptimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, i_clip_dir, j_clip_dir, pairs_i, pairs_j, pairs_area, size):\n",
    "        self.i_clip_dir = i_clip_dir\n",
    "        self.j_clip_dir = j_clip_dir\n",
    "        self.pairs_i = pairs_i\n",
    "        self.pairs_j = pairs_j\n",
    "        self.pairs_area = pairs_area\n",
    "        self.size = size\n",
    "        assert len(self.pairs_i) == len(self.pairs_j) == len(self.pairs_area)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs_i)\n",
    "    def __getitem__(self, idx):\n",
    "        i_clip = np.load(f\"{self.i_clip_dir}/{idx}_{self.size}.npy\")\n",
    "        j_clip = np.load(f\"{self.j_clip_dir}/{idx}_{self.size}.npy\")\n",
    "        mask = np.load(f\"{CLIP_MASK_DIR}/{idx}_{self.size}.npy\")\n",
    "        i_clip = torch.tensor(i_clip, dtype=torch.float32)\n",
    "        j_clip = torch.tensor(j_clip, dtype=torch.float32)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        i_idx = torch.tensor(self.pairs_i[idx])\n",
    "        j_idx = torch.tensor(self.pairs_j[idx])\n",
    "        area = torch.tensor(self.pairs_area[idx])\n",
    "        #resize to 256x256\n",
    "        return i_idx, j_idx, i_clip, j_clip, mask, area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "1e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading clips: 100%|██████████| 10142/10142 [01:59<00:00, 84.54it/s]\n"
     ]
    }
   ],
   "source": [
    "CFG = load_config(f\"{DATA_DIR}/config.py\").CALIB\n",
    "print(CFG.BATCH_SIZE)\n",
    "print(CFG.LEARNING_RATE)\n",
    "i_clips = []\n",
    "j_clips = []\n",
    "masks = []\n",
    "for i in tqdm(range(len(pairs_i)), desc=\"Loading clips\"):\n",
    "    i_clips.append(np.load(f\"{I_CLIP_DIR}/{i}_{CFG.SIZE}.npy\"))\n",
    "    j_clips.append(np.load(f\"{J_CLIP_DIR}/{i}_{CFG.SIZE}.npy\"))\n",
    "    masks.append(np.load(f\"{CLIP_MASK_DIR}/{i}_{CFG.SIZE}.npy\"))\n",
    "i_clips = np.array(i_clips)\n",
    "j_clips = np.array(j_clips)\n",
    "masks = np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18618/351468278.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pairs_i = torch.tensor(pairs_i, dtype=torch.int64)\n",
      "/tmp/ipykernel_18618/351468278.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pairs_j = torch.tensor(pairs_j, dtype=torch.int64)\n",
      "/tmp/ipykernel_18618/351468278.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pairs_area = torch.tensor(pairs_area, dtype=torch.float32)\n",
      "/tmp/ipykernel_18618/351468278.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pairs_std = torch.tensor(pairs_std, dtype=torch.float32)\n",
      "/tmp/ipykernel_18618/351468278.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  i_clips = torch.tensor(i_clips, dtype=torch.float32)\n",
      "/tmp/ipykernel_18618/351468278.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  j_clips = torch.tensor(j_clips, dtype=torch.float32)\n",
      "/tmp/ipykernel_18618/351468278.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  masks = torch.tensor(masks, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18618/914905704.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pairs_i = torch.tensor(pairs_i, dtype=torch.int64)\n",
      "/tmp/ipykernel_18618/914905704.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pairs_j = torch.tensor(pairs_j, dtype=torch.int64)\n",
      "/tmp/ipykernel_18618/914905704.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pairs_area = torch.tensor(pairs_area, dtype=torch.float32)\n",
      "/tmp/ipykernel_18618/914905704.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pairs_std = torch.tensor(pairs_std, dtype=torch.float32)\n",
      "/tmp/ipykernel_18618/914905704.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  i_clips = torch.tensor(i_clips, dtype=torch.float32)\n",
      "/tmp/ipykernel_18618/914905704.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  j_clips = torch.tensor(j_clips, dtype=torch.float32)\n",
      "/tmp/ipykernel_18618/914905704.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  masks = torch.tensor(masks, dtype=torch.float32)\n",
      "Epoch 165:   0%|          | 165/100000 [00:45<7:35:14,  3.65it/s, rel_loss=1.04, abs_loss=0.369, loss=1.41, a_mean=0.961, b_mean=0.355]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00165: reducing learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 223:   0%|          | 223/100000 [01:01<7:22:10,  3.76it/s, rel_loss=1.04, abs_loss=0.369, loss=1.41, a_mean=0.961, b_mean=0.356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00223: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 274:   0%|          | 274/100000 [01:14<7:11:59,  3.85it/s, rel_loss=1.04, abs_loss=0.369, loss=1.41, a_mean=0.961, b_mean=0.356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00274: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 291:   0%|          | 291/100000 [01:19<7:35:36,  3.65it/s, rel_loss=1.04, abs_loss=0.369, loss=1.41, a_mean=0.961, b_mean=0.356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_images = len(footprints)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pairs_i = torch.tensor(pairs_i, dtype=torch.int64)\n",
    "pairs_j = torch.tensor(pairs_j, dtype=torch.int64)\n",
    "pairs_area = torch.tensor(pairs_area, dtype=torch.float32)\n",
    "pairs_std = torch.tensor(pairs_std, dtype=torch.float32)\n",
    "i_clips = torch.tensor(i_clips, dtype=torch.float32)\n",
    "j_clips = torch.tensor(j_clips, dtype=torch.float32)\n",
    "masks = torch.tensor(masks, dtype=torch.float32)\n",
    "\n",
    "a_coefs = torch.ones(n_images, dtype=torch.float32, device=device, requires_grad=True)\n",
    "b_coefs = torch.zeros(n_images, dtype=torch.float32, device=device, requires_grad=True)\n",
    "optimizer = torch.optim.Adam([a_coefs, b_coefs], lr=1.e-1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=25, cooldown=25, verbose=True)\n",
    "\n",
    "best_loss = np.inf\n",
    "es_counter = 0\n",
    "losses = []\n",
    "for epoch in (pbar := tqdm(range(CFG.EPOCHS))):\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    optimizer.zero_grad()\n",
    "    i_clips_cal = a_coefs[pairs_i, None, None] * i_clips + b_coefs[pairs_i, None, None]\n",
    "    j_clips_cal = a_coefs[pairs_j, None, None] * j_clips + b_coefs[pairs_j, None, None]\n",
    "    rel_loss = torch.mean((i_clips_cal - j_clips_cal)**2 * masks)\n",
    "    abs_loss = 1.e-0*(torch.mean((i_clips_cal - i_clips)**2 * masks) + torch.mean((j_clips_cal - j_clips)**2 * masks))\n",
    "    #rel_loss = torch.mean(torch.abs(i_clips_cal - j_clips_cal) * masks)\n",
    "    #abs_loss = 1.e-6*torch.mean(torch.abs(i_clips_cal - i_clips)* masks) + torch.mean(torch.abs(j_clips_cal - j_clips)* masks)\n",
    "    loss = rel_loss + abs_loss\n",
    "    \n",
    "    if best_loss-loss.item() > 1.e-6:\n",
    "        best_loss = loss.item()\n",
    "        best_a_coefs = a_coefs.detach().cpu().numpy()\n",
    "        best_b_coefs = b_coefs.detach().cpu().numpy()\n",
    "        es_counter = 0\n",
    "    else:\n",
    "        es_counter += 1\n",
    "        if es_counter > 50:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    pbar.set_postfix({\"rel_loss\": rel_loss.item(), \"abs_loss\": abs_loss.item(), \"loss\": loss.item(), \"a_mean\": a_coefs.mean().item(), \"b_mean\": b_coefs.mean().item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving calibrated rasters: 100%|██████████| 450/450 [01:42<00:00,  4.39it/s]\n"
     ]
    }
   ],
   "source": [
    "recreate_dir(GEOTIFF_CAL_DIR)\n",
    "# recreate_dir(TIFF_CAL_DIR)\n",
    "for name, a, b in zip(tqdm(footprints[\"name\"].values, desc=\"Saving calibrated rasters\"), best_a_coefs, best_b_coefs):\n",
    "    geotiff = rxr.open_rasterio(f\"{GEOTIFF_OPTIM_DIR}/{name}\", masked=True)\n",
    "    geotiff.values = a * geotiff.values + b\n",
    "    geotiff.rio.to_raster(f\"{GEOTIFF_CAL_DIR}/{name}\")\n",
    "    # tiff = rxr.open_rasterio(f\"{TIFF_DIR}/{name}\", masked=True)\n",
    "    # tiff.values = a * tiff.values + b\n",
    "    # tiff.rio.to_raster(f\"{TIFF_CAL_DIR}/{name}\")\n",
    "    # os.system(f\"exiftool -tagsfromfile {TIFF_DIR}/{name} {TIFF_CAL_DIR}/{name} -overwrite_original_in_place\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = TempOptimDataset(I_CLIP_DIR, J_CLIP_DIR, pairs_i, pairs_j, pairs_area, size=CFG.SIZE)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=CFG.NUM_WORKERS)\n",
    "n_images = len(footprints)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a_coefs = torch.ones(n_images, dtype=torch.float32, device=device, requires_grad=True)\n",
    "b_coefs = torch.zeros(n_images, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([a_coefs, b_coefs], lr=CFG.LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=5, verbose=True)\n",
    "for epoch in range(CFG.EPOCHS):\n",
    "    print(f\"Epoch {epoch}/{CFG.EPOCHS}\")\n",
    "    epoch_losses = []\n",
    "    for i_idx, j_idx, i_clip, j_clip, mask, area in (pbar := tqdm(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        #compute loss\n",
    "        i_idx, j_idx, i_clip, j_clip, area = i_idx.to(device), j_idx.to(device), i_clip.to(device), j_clip.to(device), area.to(device)\n",
    "        #add singleton dimension t\n",
    "        i_clip_masked = i_clip * mask\n",
    "        j_clip_masked = j_clip * mask\n",
    "        #fig, ax = plt.subplots(1, 2)\n",
    "        #ax[0].imshow(i_clip_masked[0].detach().cpu().numpy())\n",
    "        i_clip_cal = a_coefs[i_idx][:, None, None] * i_clip + b_coefs[i_idx][:, None, None]\n",
    "        j_clip_cal = a_coefs[j_idx][:, None, None] * j_clip + b_coefs[j_idx][:, None, None]\n",
    "        i_clip_cal_masked = i_clip_cal * mask\n",
    "        j_clip_cal_masked = j_clip_cal * mask\n",
    "        #ax[1].imshow(i_clip_cal_masked[0].detach().cpu().numpy())\n",
    "        #plt.show()\n",
    "        #diff = i_clip_cal_masked - j_clip_cal_masked\n",
    "        #diff_plot = ax[2].imshow(diff[0].detach().cpu().numpy())\n",
    "        #plt.colorbar(diff_plot)\n",
    "        #plt.show()\n",
    "        rel_loss = torch.mean(torch.abs(i_clip_cal_masked - j_clip_cal_masked))\n",
    "        abs_loss = 0.0000001*(torch.mean(torch.abs(i_clip_cal_masked - i_clip_masked))+torch.mean(torch.abs(j_clip_cal_masked - j_clip_masked)))\n",
    "        loss = rel_loss + abs_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(f\"{loss.item()} = {rel_loss.item()} + {abs_loss.item()}\")\n",
    "        #set pbar description\n",
    "        pbar.set_description(f\"loss: {loss.item()}\")\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "    #print random 5 a and b coefs\n",
    "    print(f\"Epoch loss: {np.mean(epoch_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "1e-12\n",
      "Epoch 0/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7762974500656128: 100%|██████████| 5/5 [02:01<00:00, 24.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.7754704475402832\n",
      "Epoch 1/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7876381874084473: 100%|██████████| 5/5 [02:00<00:00, 24.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.7755790472030639\n",
      "Epoch 2/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7834939360618591: 100%|██████████| 5/5 [01:59<00:00, 23.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.7755393385887146\n",
      "Epoch 3/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7765941023826599: 100%|██████████| 5/5 [01:56<00:00, 23.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.7754733204841614\n",
      "Epoch 4/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7958165407180786: 100%|██████████| 5/5 [01:58<00:00, 23.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.7756572961807251\n",
      "Epoch 5/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7551568746566772: 100%|██████████| 5/5 [02:00<00:00, 24.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.7752681493759155\n",
      "Epoch 6/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.791508138179779: 100%|██████████| 5/5 [02:00<00:00, 24.05s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.7756160259246826\n",
      "Epoch 7/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7604162096977234: 100%|██████████| 5/5 [02:05<00:00, 25.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.7753184676170349\n",
      "Epoch 8/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mCFG\u001b[39m.\u001b[39mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m epoch_losses \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m i_idx, j_idx, i_clip, j_clip, mask, area \u001b[39min\u001b[39;00m (pbar \u001b[39m:=\u001b[39m tqdm(dataloader)):\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m     \u001b[39m#compute loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mTempOptimDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     15\u001b[0m     i_clip \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi_clip_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     j_clip \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mj_clip_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize\u001b[39m}\u001b[39;49;00m\u001b[39m.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m     mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mCLIP_MASK_DIR\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     i_clip \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(i_clip, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    433\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    434\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/format.py:801\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    800\u001b[0m         \u001b[39m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49mfromfile(fp, dtype\u001b[39m=\u001b[39;49mdtype, count\u001b[39m=\u001b[39;49mcount)\n\u001b[1;32m    802\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m         \u001b[39m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    804\u001b[0m         \u001b[39m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[39m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    814\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mndarray(count, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CFG = load_config(f\"{DATA_DIR}/config.py\").CALIB\n",
    "print(CFG.BATCH_SIZE)\n",
    "print(CFG.LEARNING_RATE)\n",
    "dataset = TempOptimDataset(I_CLIP_DIR, J_CLIP_DIR, pairs_i, pairs_j, pairs_area, size=CFG.SIZE)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=CFG.NUM_WORKERS)\n",
    "n_images = len(footprints)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a_coefs = torch.ones(n_images, dtype=torch.float32, device=device, requires_grad=True)\n",
    "b_coefs = torch.zeros(n_images, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([a_coefs, b_coefs], lr=CFG.LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=5, verbose=True)\n",
    "for epoch in range(CFG.EPOCHS):\n",
    "    print(f\"Epoch {epoch}/{CFG.EPOCHS}\")\n",
    "    epoch_losses = []\n",
    "    for i_idx, j_idx, i_clip, j_clip, mask, area in (pbar := tqdm(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        #compute loss\n",
    "        i_idx, j_idx, i_clip, j_clip, area = i_idx.to(device), j_idx.to(device), i_clip.to(device), j_clip.to(device), area.to(device)\n",
    "        #add singleton dimension t\n",
    "        i_clip_masked = i_clip * mask\n",
    "        j_clip_masked = j_clip * mask\n",
    "        #fig, ax = plt.subplots(1, 2)\n",
    "        #ax[0].imshow(i_clip_masked[0].detach().cpu().numpy())\n",
    "        i_clip_cal = a_coefs[i_idx][:, None, None] * i_clip + b_coefs[i_idx][:, None, None]\n",
    "        j_clip_cal = a_coefs[j_idx][:, None, None] * j_clip + b_coefs[j_idx][:, None, None]\n",
    "        i_clip_cal_masked = i_clip_cal * mask\n",
    "        j_clip_cal_masked = j_clip_cal * mask\n",
    "        #ax[1].imshow(i_clip_cal_masked[0].detach().cpu().numpy())\n",
    "        #plt.show()\n",
    "        #diff = i_clip_cal_masked - j_clip_cal_masked\n",
    "        #diff_plot = ax[2].imshow(diff[0].detach().cpu().numpy())\n",
    "        #plt.colorbar(diff_plot)\n",
    "        #plt.show()\n",
    "        rel_loss = torch.mean(torch.abs(i_clip_cal_masked - j_clip_cal_masked))\n",
    "        abs_loss = 0.0000001*(torch.mean(torch.abs(i_clip_cal_masked - i_clip_masked))+torch.mean(torch.abs(j_clip_cal_masked - j_clip_masked)))\n",
    "        loss = rel_loss + abs_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(f\"{loss.item()} = {rel_loss.item()} + {abs_loss.item()}\")\n",
    "        #set pbar description\n",
    "        pbar.set_description(f\"loss: {loss.item()}\")\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "    #print random 5 a and b coefs\n",
    "    print(f\"Epoch loss: {np.mean(epoch_losses)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading rasters: 100%|██████████| 77/77 [00:01<00:00, 43.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging...\n",
      "Merging done\n"
     ]
    }
   ],
   "source": [
    "def average_merge(merged_data, new_data, merged_mask, new_mask, index=None, roff=None, coff=None):\n",
    "    merged_data_masked = np.ma.array(merged_data, mask=merged_mask)\n",
    "    merged_data[:] = np.ma.masked_array((merged_data_masked,new_data)).mean(axis=0)\n",
    "rasters = []\n",
    "for path in tqdm(glob(f\"{GEOTIFF_CAL_DIR}/*.tiff\"), desc=\"Loading rasters\"):\n",
    "    rasters.append(rxr.open_rasterio(path, masked=True).copy())\n",
    "print(\"Merging...\")\n",
    "mosaic = merge_arrays(rasters, method=average_merge)\n",
    "mosaic.rio.to_raster(f\"{DATA_DIR}/mosaic_cal.tiff\")\n",
    "print(\"Merging done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
